```markdown
# LightGBM Reimplementation

Этот проект представляет собой реализацию алгоритма LightGBM на основе [Оригинальной статьи](https://papers.nips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf), включающую ключевые оптимизации, такие как Leaf-Wise стратегия роста дерева, использование гистограмм для разделения, а также механизмы ускорения, такие как Gradient-based One-Side Sampling (GOSS) и Exclusive Feature Bundling (EFB).


## Основные особенности

### 1. Leaf-Wise стратегия роста дерева
В отличие от традиционного подхода (level-wise, как в XGBoost и CatBoost), LightGBM использует leaf-wise стратегию роста дерева. Это позволяет строить более глубокие деревья, которые лучше разделяют данные, что повышает точность модели при одинаковой глубине.

```python
@dataclass
class DecisionTreeRegressor:
    max_depth: int
    min_samples_split: int = 2
    n_bins: int = 10  # Количество бинов для разделения на основе гистограмм

    def _fit_leaf_wise(self, X: np.ndarray, y: np.ndarray) -> Node:
        # Основная логика построения дерева по стратегии leaf-wise
```

### 2. Разделение на основе гистограмм
Для ускорения процесса обучения, вместо прямого расчета разделений для каждого возможного значения признака, используется метод гистограмм, который значительно снижает вычислительные затраты.

```python
def _best_split(self, X: np.ndarray, y: np.ndarray) -> tuple[int, float]:
    for idx in range(n_features_):
        hist, bin_edges = np.histogram(X[:, idx], bins=self.n_bins)
        # Выбор наилучшего разделения на основе расчетов MSE для каждого бинового разделения
```

### 3. Gradient-based One-Side Sampling (GOSS)
GOSS позволяет уменьшить количество данных, на которых строятся деревья, сохраняя при этом данные с наибольшими градиентами, что минимизирует потерю точности.

```python
def _goss_sampling(self, X, residuals):
    high_grad_indices = gradients >= np.percentile(gradients, 100 * (1 - self.alpha))
    low_grad_indices = ~high_grad_indices
    # Сэмплирование данных с низкими градиентами
```

### 4. Exclusive Feature Bundling (EFB)
EFB объединяет взаимно исключающие признаки в один, что позволяет уменьшить размерность признакового пространства и ускорить обучение.

```python
def _apply_efb(self, X):
    bundles = self._greedy_bundling(X, self.K)
    X_efb = self._merge_features(X, bundles)
    return X_efb
```

Алгоритм `_greedy_bundling` используется для группировки взаимно исключающих признаков в минимальное количество "связок". Поскольку задача нахождения оптимального распределения признаков NP-трудна, используется жадный алгоритм, который ищет приблизительное решение, основанное на последовательном объединении признаков с минимальными конфликтами. 

После того как признаки распределены по связкам, алгоритм `_merge_features` объединяет их в один признак, гарантируя, что исходные значения можно будет восстановить. Это достигается за счет смещения значений различных признаков внутри одной связки так, чтобы они не пересекались, что позволяет эффективно снизить сложность обучения, сохраняя точность.

## Результаты

Оптимизированная версия Gradient Boosting классификатора с использованием LightGBM механизмов показала значительное ускорение по сравнению с обычным градиентным бустингом. В частности, было достигнуто ускорение в 2 раза по времени обучения при сохранении точности модели на том же уровне.

```python
# Пример использования:
from gradient_boosting_optimized import GradientBoostingClassifierOptimized

model = GradientBoostingClassifierOptimized(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    boosting_type="goss",
    apply_efb=True
)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

## Заключение

Реализация LightGBM с использованием Leaf-Wise стратегии, GOSS и EFB демонстрирует превосходство в скорости и эффективности по сравнению с обычным градиентным бустингом, что делает его отличным выбором для задач, требующих быстрой и точной обработки больших данных.